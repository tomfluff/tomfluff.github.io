<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Yotam Sechayk </title> <meta name="author" content="Yotam Sechayk"> <meta name="description" content="All publications including journal papers, conference papers, posters/demos, and preprints."> <meta name="keywords" content="research, low-vision, hci, accessibility, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?3e214ae4e5ce871673d4453f5f8df757"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomfluff.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yotam</span> Sechayk </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">All publications including journal papers, conference papers, posters/demos, and preprints.</p> </header> <article> <h4>Search in Publications</h4> <p>Search publications using any keyword, author name, or publication venue.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>ASSETS</div> </abbr> </div> <div id="sechayk2025veasyguide" class="col-sm-8"> <div class="title">VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, <a href="https://amypavel.com/" rel="external nofollow noopener" target="_blank">Amy Pavel</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS â€™25)</em>, 2025 </div> <div class="links"> <div class="award z-depth-0">ğŸ—ï¸<span class="award-text">Honorable Mention Award</span> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663547.3746372" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Instructors often rely on visual actions such as pointing, marking, and sketching to convey information in educational presentation videos. These subtle visual cues often lack verbal descriptions, forcing low-vision (LV) learners to search for visual indicators or rely solely on audio, which can lead to missed information and increased cognitive load. To address this challenge, we conducted a co-design study with three LV participants and developed VeasyGuide, a tool that uses motion detection to identify instructor actions and dynamically highlight and magnify them. VeasyGuide produces familiar visual highlights that convey spatial context and adapt to diverse learners and content through extensive personalization and real-time visual feedback. VeasyGuide reduces visual search effort by clarifying what to look for and where to look. In an evaluation with 8 LV participants, learners demonstrated a significant improvement in detecting instructor actions, with faster response times and significantly reduced cognitive load. A separate evaluation with 8 sighted participants showed that VeasyGuide also enhanced engagement and attentiveness, suggesting its potential as a universally beneficial tool.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2025veasyguide</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Pavel, Amy and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3663547.3746372}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>ASSETS</div> </abbr> </div> <div id="mohanbabu2025taskmode" class="col-sm-8"> <div class="title">Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs</div> <div class="author"> <a href="https://ananyagm.com/" rel="external nofollow noopener" target="_blank">Ananya Gubbi Mohanbabu</a>, <em>Yotam Sechayk</em>, and <a href="https://amypavel.com/" rel="external nofollow noopener" target="_blank">Amy Pavel</a> </div> <div class="periodical"> <em>In Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS â€™25)</em>, 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663547.3746401" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Modern web interfaces are unnecessarily complex to use as they overwhelm users with excessive text and visuals unrelated to their current goals. This problem particularly impacts screen reader users (SRUs), who navigate content sequentially and may spend minutes traversing irrelevant elements before reaching desired information compared to vision users (VUs) who visually skim in seconds. We present Task Mode, a system that dynamically filters web content based on user-specified goals using large language models to identify and prioritize relevant elements while minimizing distractions. Our approach preserves page structure while offering multiple viewing modes tailored to different access needs. Our user study with 12 participants (6 VUs, 6 SRUs) demonstrates that our approach reduced task completion time for SRUs while maintaining performance for VUs, decreasing the completion time gap between groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the future, reporting that Task Mode supported completing tasks with less effort and fewer distractions. This work demonstrates how designing new interactions simultaneously for visual and non-visual access can reduce rather than reinforce accessibility disparities in future technology created by human-computer interaction researchers and practitioners.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mohanbabu2025taskmode</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mohanbabu, Ananya Gubbi and Sechayk, Yotam and Pavel, Amy}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3663547.3746401}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>ASSETS</div> </abbr> </div> <div id="pimenova2025longitudinal" class="col-sm-8"> <div class="title">A Longitudinal Autoethnography of Email Access for a Professional with Chronic Illness and ADHD: Preliminary Insights</div> <div class="author"> <a href="https://veronicapim.github.io/" rel="external nofollow noopener" target="_blank">Veronica Pimenova</a>, <em>Yotam Sechayk</em>, Fabricio Murai, Andrew Hundt, and Shiri Dori-Hacohen </div> <div class="periodical"> <em>In Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS â€™25)</em>, 2025 </div> <div class="periodical"> <small>Note: poster/demo</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663547.3759764" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Email is a foundational infrastructure of professional environments, yet for chronically ill and neurodivergent individuals, it often becomes an invisible barrier to access. We share preliminary insights from a 14-year autoethnography of a professional with chronic illness and attention-deficit/hyperactivity disorder (ADHD). We detail this professionalâ€™s iterative adaptation of mainstream email features into Mail++, their personalized workplace communication workflow for managing executive function challenges and chronic illness flares. We propose three emerging themes: (1) from hacks to assistive technology, (2) evolving access needs, and (3) toll of inaccessible systems. Based on our findings, we present initial design insights for accessible workplace communication systems. As future work in this ongoing study, we discuss a more in-depth qualitative analysis of the autoethnographic data, and formal user testing of the Mail++ approach with a population of professionals with chronic illness and ADHD to better inform the design of assistive workplace technology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pimenova2025longitudinal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Longitudinal Autoethnography of Email Access for a Professional with Chronic Illness and ADHD: Preliminary Insights}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pimenova, Veronica and Sechayk, Yotam and Murai, Fabricio and Hundt, Andrew and Dori-Hacohen, Shiri}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3663547.3759764}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{poster/demo}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#118f9c"> <div>DIS</div> </abbr> </div> <div id="drago2025improvmate" class="col-sm-8"> <div class="title">ImprovMate: Multimodal AI Assistant for Improv Actor Training</div> <div class="author"> Riccardo Drago, <em>Yotam Sechayk</em>, <a href="https://www.dogadogan.com/" rel="external nofollow noopener" target="_blank">Mustafa Doga Dogan</a>, Andrea Sanna, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Companion Publication of the 2025 ACM Designing Interactive Systems Conference (DIS â€™25 Companion)</em>, 2025 </div> <div class="periodical"> <small>Note: work-in-progress</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3715668.3736363" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">drago2025improvmate</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drago, Riccardo and Sechayk, Yotam and Dogan, Mustafa Doga and Sanna, Andrea and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ImprovMate: Multimodal AI Assistant for Improv Actor Training}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400714863}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3715668.3736363}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3715668.3736363}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Companion Publication of the 2025 ACM Designing Interactive Systems Conference (DIS '25 Companion)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{526--532}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{work-in-progress}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8c564b"> <div>SIGGRAPH</div> </abbr> </div> <div id="ji2025confidence" class="col-sm-8"> <div class="title">Confidence Estimation of Few-shot Patch-based Learning for Anime-style Colorization</div> <div class="author"> Yuexiang Ji, Akinobu Maejima, <em>Yotam Sechayk</em>, <a href="https://koyama.xyz/" rel="external nofollow noopener" target="_blank">Yuki Koyama</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters (SIGGRAPH Posters â€™25)</em>, 2025 </div> <div class="periodical"> <small>Note: poster/demo</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3721250.3742964" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In hand-drawn anime production, automatic colorization is used to boost productivity, where line drawings are automatically colored based on reference frames. However, the results sometimes include wrong color estimations, requiring artists to carefully inspect each region and correct colorsâ€”a time-consuming and labor-intensive task. To support this process, we propose a confidence estimation method that indicates the confidence level of colorization for each region of the image. Our method compares local patches in the colorized result and the reference frame.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ji2025confidence</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ji, Yuexiang and Maejima, Akinobu and Sechayk, Yotam and Koyama, Yuki and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Confidence Estimation of Few-shot Patch-based Learning for Anime-style Colorization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400715495}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3721250.3742964}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3721250.3742964}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters (SIGGRAPH Posters '25)}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{40}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Automatic colorization, Line drawing, Confidence estimation}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{SIGGRAPH Posters '25}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{poster/demo}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#ce56a9"> <div>INTERACTION</div> </abbr> </div> <div id="sechayk2024mystoryknight" class="col-sm-8"> <div class="title">MyStoryKnight: A Character-drawing Driven Storytelling System Using LLM Hallucinations</div> <div class="author"> <em>Yotam Sechayk</em>, Gabriela A. Penarska, Ingrid A. Randsalu, <a href="https://arzate-christian.github.io/" rel="external nofollow noopener" target="_blank">Christian Arzate Cruz</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³2024è«–æ–‡é›† (IPSJ INTERACTION 2024 Proceedings)</em>, Feb 2024 </div> <div class="periodical"> <small>Note: poster/demo</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Storytelling is a valuable tradition that plays a crucial role in child development, fostering creativity and a sense of agency. However, many children often consume stories passively, missing out on the opportunity to participate in the creative process. To address this, we propose a storytelling system that creates adventure-type stories with multiple branches that users can explore. We generate these interactive stories using a character drawing as input, with visual features extraction using GPT-4. By leveraging LLM hallucinations, we generate interactive stories using user feedback as a prompt. Finally, we refine the quality of the generated story through a complexity analysis algorithm. We believe that the use of a drawing as input further improves the engagement in the story and characters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024mystoryknight</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Penarska, Gabriela A. and Randsalu, Ingrid A. and Cruz, Christian Arzate and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MyStoryKnight: A Character-drawing Driven Storytelling System Using LLM Hallucinations}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³2024è«–æ–‡é›† (IPSJ INTERACTION 2024 Proceedings)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1297--1300}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{æƒ…å ±å‡¦ç†å­¦ä¼š (IPSJ)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Japan}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{poster/demo}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#d62728"> <div>CHI</div> </abbr> </div> <div id="sechayk2024smartlearn" class="col-sm-8"> <div class="title">SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, Honolulu, HI, USA, Feb 2024 </div> <div class="periodical"> <small>Note: late breaking work</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613905.3650883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In the realm of e-learning, video-based content is increasingly prevalent but brings with it unique accessibility challenges. Our research, beginning with a formative study involving 53 participants, has pinpointed the primary accessibility barriers in video-based e-learning: mismatches in user pace, complex visual arrangements leading to unclear focus, and difficulties in navigating content. To tackle these barriers, we introduced SmartLearn (SL), an innovative tool designed to enhance the accessibility of video content. SL utilizes advanced video analysis techniques to address issues of focus, navigation, and pacing, enabling users to interact with video segments more effectively through a web interface. A subsequent evaluation demonstrated that SL significantly enhances user engagement, ease of access, and learnability over existing approaches. We conclude by presenting design guidelines derived from our study, aiming to promote future efforts in research and development towards a more inclusive digital education landscape.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024smartlearn</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703317}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613905.3650883}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613905.3650883}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{294}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Accessibility, E-learning, Online learning, Temporal Accessibility, Universal Design, Video Accessibility, Visual Accessibility}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Honolulu, HI, USA}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{late breaking work}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#db6b08"> <div>ROMAN</div> </abbr> </div> <div id="sechayk2024data" class="col-sm-8"> <div class="title">Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI</div> <div class="author"> <em>Yotam Sechayk<sup>*</sup></em>, <a href="https://arzate-christian.github.io/" rel="external nofollow noopener" target="_blank">Christian Arzate Cruz<sup>*</sup></a>, <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a>, and <a href="https://www.jp.honda-ri.com/en/members/gomez-randy/" rel="external nofollow noopener" target="_blank">Randy Gomez</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* authors contributed equally"> </i> </div> <div class="periodical"> <em>In 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)</em>, Feb 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/RO-MAN60168.2024.10731438" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024data</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Cruz, Christian Arzate and Igarashi, Takeo and Gomez, Randy}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2015-2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Solid modeling;Accuracy;Three-dimensional displays;Human-robot interaction;Predictive models;Feature extraction;Data augmentation;Data models;Robustness;Robots}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/RO-MAN60168.2024.10731438}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9467bd"> <div>WISS</div> </abbr> </div> <div id="sechayk2024showme" class="col-sm-8"> <div class="title">ShowMe: å¯¾è©±çš„ãªå¼·èª¿è¡¨ç¤ºã¨æ‹¡å¤§è¡¨ç¤ºã«ã‚ˆã‚‹ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ“ãƒ‡ã‚ªã®è¦–è¦šçš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®æ”¹å–„</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In WISS 2024: ç¬¬32å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—</em>, Feb 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ“ãƒ‡ã‚ªã‚’ä½¿ã£ãŸå­¦ç¿’ã¯åºƒãä¸€èˆ¬çš„ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ï¼è¬›å¸«ã¯ï¼Œãƒ“ãƒ‡ã‚ªä½œæˆéç¨‹ã§ï¼Œã•ã¾ã–ã¾ãªè¦–è¦šçš„è£œåŠ©å‹•ä½œã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ãŒå¤šã„ï¼å…·ä½“çš„ã«ã¯ï¼Œãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¸­ã®ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ï¼Œãƒãƒ¼ã‚­ãƒ³ã‚°ï¼Œã‚¹ã‚±ãƒƒãƒãªã©ãŒï¼Œè¦–è¦šçš„è£œåŠ©å‹•ä½œã¨ã—ã¦ã‚ˆãä½¿ã‚ã‚Œã‚‹ï¼ã—ã‹ã—ï¼Œã“ã‚Œã‚‰ã®å‹•ä½œã¯è¦–è¦šçš„ã«èªè­˜ãŒé›£ã—ã„ã“ã¨ãŒå¤šãï¼Œèª¬æ˜ãŒä¸ååˆ†ã§ã‚ã‚‹ã“ã¨ãŒå¤šã„ï¼å¼±è¦–ã®å­¦ç¿’è€…ã¯ï¼Œã“ã®ã‚ˆã†ãªå‹•ä½œã«è¿½å¾“ã™ã‚‹ãŸã‚ã«ï¼Œå¸¸ã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã‚’æ¢ç´¢ã™ã‚‹å¿…è¦ãŒã‚ã‚Šï¼Œãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ç–²åŠ´ã«ã¤ãªãŒã£ã¦ã„ã‚‹ï¼æˆ‘ã€…ã¯ï¼Œã“ã®å•é¡Œã‚’ç†è§£ã—è§£æ±ºã™ã‚‹ãŸã‚ã«ï¼Œ3 äººã®å¼±è¦–ãƒ¦ãƒ¼ã‚¶ã¨ãƒ¦ãƒ¼ã‚¶å‚åŠ å‹ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å®Ÿæ–½ã—ï¼Œãã®çµæœã«ã‚‚ã¨ã¥ãï¼Œè¬›å¸«ã®è¦–è¦šçš„è£œåŠ©å‹•ä½œã‚’å¼·èª¿è¡¨ç¤ºã—ï¼Œæ‹¡å¤§è¡¨ç¤ºã™ã‚‹ãƒ„ãƒ¼ãƒ« ShowMe ã‚’é–‹ç™ºã—ãŸï¼ShowMe ã¯ï¼Œå¼±è¦–ãƒ¦ãƒ¼ã‚¶ãŒãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã§ãã‚‹ã‚ˆã†ã«æ”¯æ´ã—ï¼Œç–²åŠ´ã¨ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è»½æ¸›ã™ã‚‹ï¼</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024showme</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ShowMe: å¯¾è©±çš„ãªå¼·èª¿è¡¨ç¤ºã¨æ‹¡å¤§è¡¨ç¤ºã«ã‚ˆã‚‹ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ“ãƒ‡ã‚ªã®è¦–è¦šçš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®æ”¹å–„}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WISS 2024: ç¬¬32å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{æƒ…å ±å‡¦ç†å­¦ä¼š (IPSJ)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Japan}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{137--145}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{japanese}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9467bd"> <div>WISS</div> </abbr> </div> <div id="sechayk2023smartreplay" class="col-sm-8"> <div class="title">Smart Replay: eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ç”»ã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ»æ™‚é–“çš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In WISS 2023: ç¬¬31å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—</em>, Feb 2023 </div> <div class="links"> <div class="award z-depth-0">ğŸ“œ<span class="award-text">Helpfeel Award for Corporate Excellence</span> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ï¼Œæ•™æã¸ã®å¹…åºƒã„ã‚¢ã‚¯ã‚»ã‚¹ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ï¼ã—ã‹ã—ï¼Œå‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å¤šç”¨ã™ã‚‹ã“ã¨ã¯ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã«å¤§ããªèª²é¡Œã‚’ã‚‚ãŸã‚‰ã™ï¼å¤šæ§˜ãªå‚åŠ è€…ã‚’å¯¾è±¡ã¨ã—ãŸäºˆå‚™çš„èª¿æŸ»ã«åŸºã¥ãï¼Œe ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ“ãƒ‡ã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å­˜åœ¨ã™ã‚‹ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®éšœå£ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ï¼ã“ã‚Œã‚‰ã«ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç†è§£é€Ÿåº¦ã¨å‹•ç”»é€Ÿåº¦ã®ä¸ä¸€è‡´ï¼Œã©ã“ã«æ³¨æ„ã‚’å‘ã‘ã¦ã‚ˆã„ã‹ã‚ã‹ã‚‰ãªã„è¦–è¦šçš„è¤‡é›‘ã•ï¼ŒãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®ã—ã«ãã•ãªã©ãŒå«ã¾ã‚Œã‚‹ï¼ã¾ãŸï¼Œæˆ‘ã€…ã®èª¿æŸ»çµæœã¯ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å•é¡ŒãŒï¼Œéšœå®³ã®ã‚ã‚‹åˆ©ç”¨è€…ã¨ãªã„åˆ©ç”¨è€…ã®ä¸¡æ–¹ã«å½±éŸ¿ã‚’åŠã¼ã™ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ï¼ã•ã‚‰ã«ï¼Œæ—¢å­˜ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚Šï¼Œã•ã•ã‚‰ãªã‚‹å¯¾å¿œãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ï¼ãã“ã§æˆ‘ã€…ã¯ï¼Œe ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹ã€ŒSmart Replayã€ã‚’ææ¡ˆã™ã‚‹ï¼ç§ãŸã¡ã®ãƒ„ãƒ¼ãƒ«ã¯ï¼Œå­¦ç¿’ãƒ“ãƒ‡ã‚ªã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ãŸåˆ†æã‚’è¡Œã„ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ–ãƒ«ãªå†ç”Ÿã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ï¼è¦–è¦šçš„åˆ†é‡ã¨æ™‚é–“çš„åˆ†é‡ã®ä¸¡æ–¹ã‚’å¼·åŒ–ã—ãŸãƒ“ãƒ‡ã‚ªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾©ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ï¼</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2023smartreplay</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Smart Replay: eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ç”»ã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ»æ™‚é–“çš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WISS 2023: ç¬¬31å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{25--33}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{æƒ…å ±å‡¦ç†å­¦ä¼š (IPSJ)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Japan}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{japanese}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 Yotam Sechayk. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?230637657ac0b42e92d76e2b4c1b4764"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>