---
---

@inproceedings{sechayk2023smartreplay,
  author    = {Yotam Sechayk and Ariel Shamir and Takeo Igarashi},
  title     = {Smart Replay: eラーニング動画における視覚的・時間的アクセシビリティの向上},
  booktitle = {WISS 2023: 第31回インタラクティブシステムとソフトウェアに関するワークショップ},
  abstract  = {eラーニングは，教材への幅広いアクセスを可能にすることを目的としている．しかし，動画コンテンツを多用することは，アクセシビリティに大きな課題をもたらす．多様な参加者を対象とした予備的調査に基づき，e ラーニングのビデオコンテンツに存在するアクセシビリティの障壁を明らかにする．これらには，ユーザーの理解速度と動画速度の不一致，どこに注意を向けてよいかわからない視覚的複雑さ，ナビゲーションのしにくさなどが含まれる．また，我々の調査結果は，アクセシビリティの問題が，障害のある利用者とない利用者の両方に影響を及ぼすことを示している．さらに，既存のアクセシビリティツールには限界があり，ささらなる対応が必要であることを示している．そこで我々は，e ラーニングアクセシビリティツールである「Smart Replay」を提案する．私たちのツールは，学習ビデオのビジュアルとコンテンツに基づいた分析を行い，アクセシブルな再生オプションを生成する．視覚的分野と時間的分野の両方を強化したビデオセクションの復習を可能にする．},
  year      = {2023},
  pages     = {25--33},
  publisher = {情報処理学会 (IPSJ)},
  abbr      = {WISS},
  address   = {Japan},
  language  = {japanese},
  bibtex_show = {true},
  award = {Helpfeel Award for Corporate Excellence},
  selected = {true},
  pdf = {smart-replay.pdf},
  video ={https://www.wiss.org/WISS2023Proceedings/data/04.mp4}
}
@inproceedings{sechayk2024mystoryknight,
  author    = {Yotam Sechayk and Gabriela A. Penarska and Ingrid A. Randsalu and Christian Arzate Cruz and Takeo Igarashi},
  title     = {MyStoryKnight: A Character-drawing Driven Storytelling System Using LLM Hallucinations},
  booktitle = {インタラクション2024論文集 (IPSJ INTERACTION 2024 Proceedings)},
  abstract  = {Storytelling is a valuable tradition that plays a crucial role in child development, fostering creativity and a sense of agency. However, many children often consume stories passively, missing out on the opportunity to participate in the creative process. To address this, we propose a storytelling system that creates adventure-type stories with multiple branches that users can explore. We generate these interactive stories using a character drawing as input, with visual features extraction using GPT-4. By leveraging LLM hallucinations, we generate interactive stories using user feedback as a prompt. Finally, we refine the quality of the generated story through a complexity analysis algorithm. We believe that the use of a drawing as input further improves the engagement in the story and characters.},
  year      = {2024},
  month     = feb,
  pages     = {1297--1300},
  publisher = {情報処理学会 (IPSJ)},
  abbr      = {INTERACTION},
  address   = {Japan},
  note      = {poster/demo},
  language  = {english},
  bibtex_show = {true},
  pdf = {mystoryknight.pdf},
  demo = {https://tomfluff.github.io/MyStoryKnight/}
}
@inproceedings{sechayk2024smartlearn,
author = {Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo},
title = {SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
abbr      = {CHI},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650883},
doi = {10.1145/3613905.3650883},
abstract = {In the realm of e-learning, video-based content is increasingly prevalent but brings with it unique accessibility challenges. Our research, beginning with a formative study involving 53 participants, has pinpointed the primary accessibility barriers in video-based e-learning: mismatches in user pace, complex visual arrangements leading to unclear focus, and difficulties in navigating content. To tackle these barriers, we introduced SmartLearn (SL), an innovative tool designed to enhance the accessibility of video content. SL utilizes advanced video analysis techniques to address issues of focus, navigation, and pacing, enabling users to interact with video segments more effectively through a web interface. A subsequent evaluation demonstrated that SL significantly enhances user engagement, ease of access, and learnability over existing approaches. We conclude by presenting design guidelines derived from our study, aiming to promote future efforts in research and development towards a more inclusive digital education landscape.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {294},
numpages = {11},
keywords = {Accessibility, E-learning, Online learning, Temporal Accessibility, Universal Design, Video Accessibility, Visual Accessibility},
location = {Honolulu, HI, USA},
language = {english},
note = {late breaking work},
series = {CHI EA '24},
  bibtex_show = {true},
  selected = {true},
  pdf = {smartlearn.pdf},
  video = {https://dl.acm.org/doi/suppl/10.1145/3613905.3650883/suppl_file/3613905.3650883-talk-video.mp4}
}
@inproceedings{sechayk2024data,
  author={Yotam Sechayk* and Christian Arzate Cruz* and Takeo Igarashi and Randy Gomez},
  annotation={* authors contributed equally},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}, 
  title={Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI}, 
  abstract={Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.},
  publisher={IEEE},
  abbr      = {ROMAN},
  year={2024},
  volume={},
  number={},
  pages={2015-2022},
  keywords={Solid modeling;Accuracy;Three-dimensional displays;Human-robot interaction;Predictive models;Feature extraction;Data augmentation;Data models;Robustness;Robots},
  doi={10.1109/RO-MAN60168.2024.10731438},
  language={english},
  bibtex_show = {true},
  selected = {true},
  pdf = {3dmm-aug.pdf},
}
@inproceedings{sechayk2024showme,
  author    = {Yotam Sechayk and Ariel Shamir and Takeo Igarashi},
  title     = {ShowMe: 対話的な強調表示と拡大表示によるプレゼンテーションビデオの視覚的アクセシビリティの改善},
  booktitle = {WISS 2024: 第32回インタラクティブシステムとソフトウェアに関するワークショップ},
  abstract  = {プレゼンテーションビデオを使った学習は広く一般的に行われている．講師は，ビデオ作成過程で，さまざまな視覚的補助動作を活用することが多い．具体的には，プレゼンテーション中のポインティング，マーキング，スケッチなどが，視覚的補助動作としてよく使われる．しかし，これらの動作は視覚的に認識が難しいことが多く，説明が不十分であることが多い．弱視の学習者は，このような動作に追従するために，常にプレゼンテーションのフレーム内を探索する必要があり，フラストレーションと疲労につながっている．我々は，この問題を理解し解決するために，3 人の弱視ユーザとユーザ参加型デザインを実施し，その結果にもとづき，講師の視覚的補助動作を強調表示し，拡大表示するツール ShowMe を開発した．ShowMe は，弱視ユーザがプレゼンテーションをフォローできるように支援し，疲労とフラストレーションを軽減する．},
  year      = {2024},
  publisher = {情報処理学会 (IPSJ)},
  abbr      = {WISS},
  address   = {Japan},
  pages     = {137--145},
  language  = {japanese},
  bibtex_show = {true},
  pdf = {showme.pdf},
  video = {https://www.wiss.org/WISS2024Proceedings/data/paper//19.mp4}
}
@inproceedings{sechayk2025veasyguide,
  title = {VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos},
  author = {Yotam Sechayk and Ariel Shamir and Amy Pavel and Takeo Igarashi},
  booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)},
  abstract = {Instructors often rely on visual actions such as pointing, marking, and sketching to convey information in educational presentation videos. These subtle visual cues often lack verbal descriptions, forcing low-vision (LV) learners to search for visual indicators or rely solely on audio, which can lead to missed information and increased cognitive load. To address this challenge, we conducted a co-design study with three LV participants and developed VeasyGuide, a tool that uses motion detection to identify instructor actions and dynamically highlight and magnify them. VeasyGuide produces familiar visual highlights that convey spatial context and adapt to diverse learners and content through extensive personalization and real-time visual feedback. VeasyGuide reduces visual search effort by clarifying what to look for and where to look. In an evaluation with 8 LV participants, learners demonstrated a significant improvement in detecting instructor actions, with faster response times and significantly reduced cognitive load. A separate evaluation with 8 sighted participants showed that VeasyGuide also enhanced engagement and attentiveness, suggesting its potential as a universally beneficial tool.},
  publisher = {Association for Computing Machinery},
  abbr      = {ASSETS},
  address = {Denver, CO, USA},
  year = {2025},
  doi = {10.1145/3663547.3746372},
  language = {english},
  bibtex_show = {true},
  selected = {true},
  award = {Honorable Mention Award},
  arxiv = {https://arxiv.org/abs/2507.21837},
  website = {https://veasyguide.github.io/}
}
@inproceedings{mohanbabu2025taskmode,
  title = {Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs},
  author = {Ananya Gubbi Mohanbabu and Yotam Sechayk and Amy Pavel},
  booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)},
  abstract = {Modern web interfaces are unnecessarily complex to use as they overwhelm users with excessive text and visuals unrelated to their current goals. This problem particularly impacts screen reader users (SRUs), who navigate content sequentially and may spend minutes traversing irrelevant elements before reaching desired information compared to vision users (VUs) who visually skim in seconds. We present Task Mode, a system that dynamically filters web content based on user-specified goals using large language models to identify and prioritize relevant elements while minimizing distractions. Our approach preserves page structure while offering multiple viewing modes tailored to different access needs. Our user study with 12 participants (6 VUs, 6 SRUs) demonstrates that our approach reduced task completion time for SRUs while maintaining performance for VUs, decreasing the completion time gap between groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the future, reporting that Task Mode supported completing tasks with less effort and fewer distractions. This work demonstrates how designing new interactions simultaneously for visual and non-visual access can reduce rather than reinforce accessibility disparities in future technology created by human-computer interaction researchers and practitioners.},
  publisher = {Association for Computing Machinery},
  abbr      = {ASSETS},
  address = {Denver, CO, USA},
  year = {2025},
  doi = {10.1145/3663547.3746401},
  language = {english},
  bibtex_show = {true},
  arxiv = {https://arxiv.org/abs/2507.14769v1}
}
@inproceedings{pimenova2025longitudinal,
  title = {A Longitudinal Autoethnography of Email Access for a Professional with Chronic Illness and ADHD: Preliminary Insights},
  author = {Veronica Pimenova and Yotam Sechayk and Fabricio Murai and Andrew Hundt and Shiri Dori-Hacohen},
  booktitle = {Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)},
  abstract = {Email is a foundational infrastructure of professional environments, yet for chronically ill and neurodivergent individuals, it often becomes an invisible barrier to access. We share preliminary insights from a 14-year autoethnography of a professional with chronic illness and attention-deficit/hyperactivity disorder (ADHD). We detail this professional's iterative adaptation of mainstream email features into Mail++, their personalized workplace communication workflow for managing executive function challenges and chronic illness flares. We propose three emerging themes: (1) from hacks to assistive technology, (2) evolving access needs, and (3) toll of inaccessible systems. Based on our findings, we present initial design insights for accessible workplace communication systems. As future work in this ongoing study, we discuss a more in-depth qualitative analysis of the autoethnographic data, and formal user testing of the Mail++ approach with a population of professionals with chronic illness and ADHD to better inform the design of assistive workplace technology.},
  publisher = {Association for Computing Machinery},
  abbr      = {ASSETS},
  address = {Denver, CO, USA},
  year = {2025},
  doi = {10.1145/3663547.3759764},
  language = {english},
  note = {poster/demo},
  bibtex_show = {true},
  pdf = {email-autoethno.pdf}
}
@inproceedings{drago2025improvmate,
author = {Riccardo Drago and Yotam Sechayk and Mustafa Doga Dogan and Andrea Sanna and Takeo Igarashi},
title = {ImprovMate: Multimodal AI Assistant for Improv Actor Training},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
abbr      = {DIS},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715668.3736363},
doi = {10.1145/3715668.3736363},
abstract = {Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.},
booktitle = {Companion Publication of the 2025 ACM Designing Interactive Systems Conference (DIS '25 Companion)},
pages = {526--532},
numpages = {7},
language = {english},
note = {work-in-progress},
  bibtex_show = {true},
  selected = {true},
  pdf = {improvmate.pdf},
  website = {https://improvmate.github.io/},
  demo = {https://tomfluff.github.io/ImprovMate/}
}
@inproceedings{ji2025confidence,
author = {Yuexiang Ji and Akinobu Maejima and Yotam Sechayk and Yuki Koyama and Takeo Igarashi},
title = {Confidence Estimation of Few-shot Patch-based Learning for Anime-style Colorization},
year = {2025},
isbn = {9798400715495},
publisher = {Association for Computing Machinery},
abbr      = {SIGGRAPH},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721250.3742964},
doi = {10.1145/3721250.3742964},
abstract = {In hand-drawn anime production, automatic colorization is used to boost productivity, where line drawings are automatically colored based on reference frames. However, the results sometimes include wrong color estimations, requiring artists to carefully inspect each region and correct colors—a time-consuming and labor-intensive task. To support this process, we propose a confidence estimation method that indicates the confidence level of colorization for each region of the image. Our method compares local patches in the colorized result and the reference frame.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters (SIGGRAPH Posters '25)},
articleno = {40},
numpages = {2},
keywords = {Automatic colorization, Line drawing, Confidence estimation},
series = {SIGGRAPH Posters '25},
language = {english},
note = {poster/demo},
  bibtex_show = {true}
}