<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yotam Sechayk </title> <meta name="author" content="Yotam Sechayk"> <meta name="description" content="Yotam Sechayk is a PhD candidate at the University of Tokyo, working on HCI and accessibility research. "> <meta name="keywords" content="research, low-vision, hci, accessibility, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?1fefb6021bf36010f25ea1cef24af84e"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?53a094b51ed1d1e025731eb00d240058" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?3e214ae4e5ce871673d4453f5f8df757"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomfluff.github.io/"> <script src="/assets/js/theme.js?f2531f05c6f8e1622518f4f5a1e385b1"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?46af317e693b09921dcb92261d123fbc" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">Ctrl K <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yotam</span> Sechayk </h1> <p class="desc">ğŸ“ PhD Candidate @ <a href="https://www.u-tokyo.ac.jp/en/" rel="external nofollow noopener" target="_blank">The University of Tokyo</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic_color.jpg?d715cf42f22541cd8b27d2c76e6da32d" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic_color.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p style="font-weight: bold;">HCI, Accessibility, AI</p> <i>ysechayk (at) acm (dot) org</i> </div> </div> <div class="clearfix"> <p>I am a Ph.D. candidate at The University of Tokyo, advised by <a href="http://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a>, and co-advised by <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a> of Reichman University.</p> <p>In 2018 I received a BS in Computer Science and Film Studies as a double-major from Tel-Aviv University, followed by 3+ years as a software engineer at Cadence Design Systems. In 2022, I relocated to Japan to pursue an MSc in Creative Informatics at The University of Tokyo, graduating in 2024.</p> <p>My main research interest is accessibility, where I develop and evaluate solutions to help low-vision people more easily use their residual vision to access content. As a <strong>low-vision person</strong> myself, I am motivated to improve visual access to education, media, and everyday-tasks.</p> <p>Before starting my BS I solo-traveled for one year around the world visiting 14 countries. It was an amazing journey where everyday is a new adventure. During this one year I managed to get my ğŸ¥½ PADI advanced diving licence in Thailand, ğŸª‚ skydive over <a href="https://en.wikipedia.org/wiki/Lake_Taup%C5%8D" rel="external nofollow noopener" target="_blank">Lake TaupÅ</a> in New-Zealand, and ğŸš— hitchhike over 3000 km from Vancouver, Canada to Anchorage, Alaska.</p> <blockquote class="stylish-quote"> I shall pass this way but once; any good that I can do or any kindness I can show to any human being; let me do it now. Let me not defer nor neglect it, for I shall not pass this way again. <cite>Etienne de Grellet</cite> </blockquote> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 17, 2025</th> <td> Started a 2-month research visit to the University of Washington at the <a href="https://makeabilitylab.cs.washington.edu/" rel="external nofollow noopener" target="_blank">Makeability Lab</a> with <a href="https://jonfroehlich.github.io/" rel="external nofollow noopener" target="_blank">Prof. Jon Froehlich</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 29, 2025</th> <td> Our work <em>VeasyGuide</em> received an Honorable Mention Award at ASSETS 2025! ğŸ‰ </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 18, 2025</th> <td> Our work <em>â€œVeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videosâ€</em> was accepted to ASSETS 2025! âœ¨ </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2025</th> <td> Received the <strong>BeyondAI SoftBank</strong> research grant for supporting visual access of low-vision people! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 14, 2024</th> <td> Presented our work <em>ShowMe</em> at WISS 2024 conference. ğŸ—£ï¸ </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">Latest Posts</a> </h2> <div class="news"> <p><i>No available posts to display.</i></p> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1f77b4"> <div>ASSETS</div> </abbr> </div> <div id="sechayk2025veasyguide" class="col-sm-8"> <div class="title">VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, <a href="https://amypavel.com/" rel="external nofollow noopener" target="_blank">Amy Pavel</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS â€™25)</em>, 2025 </div> <div class="links"> <div class="award z-depth-0">ğŸ—ï¸<span class="award-text">Honorable Mention Award</span> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663547.3746372" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Instructors often rely on visual actions such as pointing, marking, and sketching to convey information in educational presentation videos. These subtle visual cues often lack verbal descriptions, forcing low-vision (LV) learners to search for visual indicators or rely solely on audio, which can lead to missed information and increased cognitive load. To address this challenge, we conducted a co-design study with three LV participants and developed VeasyGuide, a tool that uses motion detection to identify instructor actions and dynamically highlight and magnify them. VeasyGuide produces familiar visual highlights that convey spatial context and adapt to diverse learners and content through extensive personalization and real-time visual feedback. VeasyGuide reduces visual search effort by clarifying what to look for and where to look. In an evaluation with 8 LV participants, learners demonstrated a significant improvement in detecting instructor actions, with faster response times and significantly reduced cognitive load. A separate evaluation with 8 sighted participants showed that VeasyGuide also enhanced engagement and attentiveness, suggesting its potential as a universally beneficial tool.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2025veasyguide</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Pavel, Amy and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Denver, CO, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3663547.3746372}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#118f9c"> <div>DIS</div> </abbr> </div> <div id="drago2025improvmate" class="col-sm-8"> <div class="title">ImprovMate: Multimodal AI Assistant for Improv Actor Training</div> <div class="author"> Riccardo Drago, <em>Yotam Sechayk</em>, <a href="https://www.dogadogan.com/" rel="external nofollow noopener" target="_blank">Mustafa Doga Dogan</a>, Andrea Sanna, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Companion Publication of the 2025 ACM Designing Interactive Systems Conference (DIS â€™25 Companion)</em>, 2025 </div> <div class="periodical"> <small>Note: work-in-progress</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3715668.3736363" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">drago2025improvmate</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drago, Riccardo and Sechayk, Yotam and Dogan, Mustafa Doga and Sanna, Andrea and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ImprovMate: Multimodal AI Assistant for Improv Actor Training}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400714863}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3715668.3736363}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3715668.3736363}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Companion Publication of the 2025 ACM Designing Interactive Systems Conference (DIS '25 Companion)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{526--532}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{work-in-progress}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#d62728"> <div>CHI</div> </abbr> </div> <div id="sechayk2024smartlearn" class="col-sm-8"> <div class="title">SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, Honolulu, HI, USA, 2024 </div> <div class="periodical"> <small>Note: late breaking work</small> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613905.3650883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In the realm of e-learning, video-based content is increasingly prevalent but brings with it unique accessibility challenges. Our research, beginning with a formative study involving 53 participants, has pinpointed the primary accessibility barriers in video-based e-learning: mismatches in user pace, complex visual arrangements leading to unclear focus, and difficulties in navigating content. To tackle these barriers, we introduced SmartLearn (SL), an innovative tool designed to enhance the accessibility of video content. SL utilizes advanced video analysis techniques to address issues of focus, navigation, and pacing, enabling users to interact with video segments more effectively through a web interface. A subsequent evaluation demonstrated that SL significantly enhances user engagement, ease of access, and learnability over existing approaches. We conclude by presenting design guidelines derived from our study, aiming to promote future efforts in research and development towards a more inclusive digital education landscape.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024smartlearn</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SmartLearn: Visual-Temporal Accessibility for Slide-based e-learning Videos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703317}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613905.3650883}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613905.3650883}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{294}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Accessibility, E-learning, Online learning, Temporal Accessibility, Universal Design, Video Accessibility, Visual Accessibility}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Honolulu, HI, USA}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{late breaking work}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '24}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#db6b08"> <div>ROMAN</div> </abbr> </div> <div id="sechayk2024data" class="col-sm-8"> <div class="title">Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI</div> <div class="author"> <em>Yotam Sechayk<sup>*</sup></em>, <a href="https://arzate-christian.github.io/" rel="external nofollow noopener" target="_blank">Christian Arzate Cruz<sup>*</sup></a>, <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a>, and <a href="https://www.jp.honda-ri.com/en/members/gomez-randy/" rel="external nofollow noopener" target="_blank">Randy Gomez</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* authors contributed equally"> </i> </div> <div class="periodical"> <em>In 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/RO-MAN60168.2024.10731438" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2024data</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Cruz, Christian Arzate and Igarashi, Takeo and Gomez, Randy}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data Augmentation for 3DMM-based Arousal-Valence Prediction for HRI}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2015-2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Solid modeling;Accuracy;Three-dimensional displays;Human-robot interaction;Predictive models;Feature extraction;Data augmentation;Data models;Robustness;Robots}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/RO-MAN60168.2024.10731438}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#9467bd"> <div>WISS</div> </abbr> </div> <div id="sechayk2023smartreplay" class="col-sm-8"> <div class="title">Smart Replay: eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ç”»ã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ»æ™‚é–“çš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š</div> <div class="author"> <em>Yotam Sechayk</em>, <a href="https://faculty.runi.ac.il/arik/site/index.asp" rel="external nofollow noopener" target="_blank">Ariel Shamir</a>, and <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/" rel="external nofollow noopener" target="_blank">Takeo Igarashi</a> </div> <div class="periodical"> <em>In WISS 2023: ç¬¬31å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—</em>, 2023 </div> <div class="links"> <div class="award z-depth-0">ğŸ“œ<span class="award-text">Helpfeel Award for Corporate Excellence</span> </div> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ï¼Œæ•™æã¸ã®å¹…åºƒã„ã‚¢ã‚¯ã‚»ã‚¹ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ï¼ã—ã‹ã—ï¼Œå‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å¤šç”¨ã™ã‚‹ã“ã¨ã¯ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã«å¤§ããªèª²é¡Œã‚’ã‚‚ãŸã‚‰ã™ï¼å¤šæ§˜ãªå‚åŠ è€…ã‚’å¯¾è±¡ã¨ã—ãŸäºˆå‚™çš„èª¿æŸ»ã«åŸºã¥ãï¼Œe ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ“ãƒ‡ã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å­˜åœ¨ã™ã‚‹ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®éšœå£ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ï¼ã“ã‚Œã‚‰ã«ã¯ï¼Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç†è§£é€Ÿåº¦ã¨å‹•ç”»é€Ÿåº¦ã®ä¸ä¸€è‡´ï¼Œã©ã“ã«æ³¨æ„ã‚’å‘ã‘ã¦ã‚ˆã„ã‹ã‚ã‹ã‚‰ãªã„è¦–è¦šçš„è¤‡é›‘ã•ï¼ŒãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®ã—ã«ãã•ãªã©ãŒå«ã¾ã‚Œã‚‹ï¼ã¾ãŸï¼Œæˆ‘ã€…ã®èª¿æŸ»çµæœã¯ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å•é¡ŒãŒï¼Œéšœå®³ã®ã‚ã‚‹åˆ©ç”¨è€…ã¨ãªã„åˆ©ç”¨è€…ã®ä¸¡æ–¹ã«å½±éŸ¿ã‚’åŠã¼ã™ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ï¼ã•ã‚‰ã«ï¼Œæ—¢å­˜ã®ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚Šï¼Œã•ã•ã‚‰ãªã‚‹å¯¾å¿œãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ï¼ãã“ã§æˆ‘ã€…ã¯ï¼Œe ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹ã€ŒSmart Replayã€ã‚’ææ¡ˆã™ã‚‹ï¼ç§ãŸã¡ã®ãƒ„ãƒ¼ãƒ«ã¯ï¼Œå­¦ç¿’ãƒ“ãƒ‡ã‚ªã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ãŸåˆ†æã‚’è¡Œã„ï¼Œã‚¢ã‚¯ã‚»ã‚·ãƒ–ãƒ«ãªå†ç”Ÿã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ï¼è¦–è¦šçš„åˆ†é‡ã¨æ™‚é–“çš„åˆ†é‡ã®ä¸¡æ–¹ã‚’å¼·åŒ–ã—ãŸãƒ“ãƒ‡ã‚ªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å¾©ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ï¼</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sechayk2023smartreplay</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sechayk, Yotam and Shamir, Ariel and Igarashi, Takeo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Smart Replay: eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ç”»ã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ»æ™‚é–“çš„ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{WISS 2023: ç¬¬31å›ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã«é–¢ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{25--33}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{æƒ…å ±å‡¦ç†å­¦ä¼š (IPSJ)}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Japan}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{japanese}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%73%65%63%68%61%79%6B@%61%63%6D.%6F%72%67" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/tomfluff" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yotams" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0009-0002-5286-0080" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=PzDcxYoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 Yotam Sechayk. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?c999e8c5281874b7534e3352f835d4c3" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?b977fe0c21b2118ed853308b1b923969"></script> <script src="/assets/js/no_defer.js?699fa7cbe3b29f831db7d5250ba3203a"></script> <script defer src="/assets/js/common.js?d3a25b46bbd2e0a751a27b173abc6e5f"></script> <script defer src="/assets/js/copy_code.js?fff63901a03063094790ffbfd4bc0cb4" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?25eff8ff4144a010e4ad7b31403102cf"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?04761245f225e3ad9e5e3f875d4e1074"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?b78cb42895d74e4fd6de6f633edcf181" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?230637657ac0b42e92d76e2b4c1b4764"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?ade73d6d60912d119f9c9347bc176630"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?f74bfa9a88ab862fb9df1c46146b7b7d"></script> </body> </html>